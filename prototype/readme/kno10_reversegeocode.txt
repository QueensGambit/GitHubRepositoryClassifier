b'# Incredibly Fast Reverse Geocoding with OpenStreetMap Data\n\n[![DOI:10.5282/ubm/data.61](https://img.shields.io/badge/DOI-10.5282%2Fubm%2Fdata.61-brightgreen.svg)](http://dx.doi.org/10.5282/ubm/data.61)\n\nThe goal of this project is to build a primitive but incredibly fast\nreverse geocoding (coordinates to location lookup) system.\n\nThe basic principle is simple:\n- extract polygons of each administrative region on OpenStreetMap (OSM)\n- build a lookup map that can be stored reasonably\n- to geocode coordinates, look them up in the map\n\nThe map will look roughly like this\n(this is a visualization of the lookup map with 0.5 degree resolution\n-- the finest has the 1000 fold resolution!):\n\n![Visualization](data/osm-20151130-0.5-2.png)\n\nHowever, things with OSM are not as easy as one may expect:\n\n- OSM is a *lot* of data. It may use up all your memory easily.\n  We are talking of 2.7 *billion* nodes for the planet file,\n  and 270 million ways and 3.1 million relations.\n  32 bits are not enough to store the IDs.\n- The dump file is not well suited for random access. Instead, you\n  need to process it in sequence. There are optimized data structures in\n  the PBF file format that exploit delta compression; and strings are shared\n  via a dictionary - and trust me, you don\'t want to process the XML dump\n  using a DOM parser and XPath either...\n- To reduce maintainance, OSM is heavily designed to share nodes and ways for\n  multiple purposes. In particular for boundaries a single way can be used\n  multiple times, such as denote the border of two countries, and participating\n  in the polygons of countries, counties, cities at the border, ...\n- Assembling polygons in OSM is quite tricky; and often polygons are incomplete\n  (see also the documentation on [OSM multipolygons](http://wiki.openstreetmap.org/wiki/Relation:multipolygon))\n\nMy first prototypes in Python were running out of memory on the full data set;\nreducing the data set via Osmosis did not work, as it led to missing ways. So I\nneeded to carefully build this in Java, to conserve memory. Osmosis reads and\nwrites data multiple times (to large temporary files) - I decided to design my\napproach around reading the input data multiple times instead, even if this\nmeans re-reading data unnecessarily, at the benefit of not having to write\nlarge temporary files.\n\n## Index Query\n\nQuering the index is easy. It requires a single class,\n[ReverseGeocoder](src/main/java/com/kno10/reversegeocode/query/ReverseGeocoder.java),\nand a data file ([pregenerated data available under the ODbL license](data/))\n\n**This class only depends on the JDK, on no other classes or packages.**\n\nIn particular, **you do not need a Mavenized package of this project**. For this reason,\nI have also chosen a very permissive 2-clause BSD license for the lookup code.\n\n    ReverseGeocoder rgc = new ReverseGeocoder(filename);\n    // ...\n    String[] metadata = rgc.lookup(longitude, latitude);\n\nPay attention to the order of longitude and latitude! The order of these two values\nis inconsistent across different applications. This has been a mess historically,\nand we are not going to fix this here. We chose longitude, latitude because it seems\nmore intuitive to us to use x, y as commonly seen on a map. This is also the\norder used by [GeoJSON](http://geojson.org/). (Plus, the run length\nencoding supposedly works better this direction.)\n\nEach metadata entry is a tab separated array, as generated by the parser from the OSM file.\nThe exact columns are easy to vary. For the provided data files, there is\n[documentation in the data folder](data/).\n\n## Performance\n\nSo how "incredibly fast" is it?\n\nI\'ve added JMH unit tests to benchmark the throughput on a i7-3770 CPU with 3.4 GHz.\nValues are given in microseconds (1us = 1e-6 s) per operation, and operations per microsecond.\n\n| Method                             | Resolution | Runtime      | Throughput    |\n|:-----------------------------------|-----------:|-------------:|--------------:|\n| Lookup with caching of entities    |      0.5-2 |  0.231 us/op |  4.316 ops/us |\n| Lookup with caching of entities    |     0.05-2 |  0.575 us/op |  1.706 ops/us |\n| Lookup with caching of entities    |    0.005-2 |  0.971 us/op |  1.052 ops/us |\n| Lookup with caching of entities    |   0.0005-2 |  1.562 us/op |  0.639 ops/us |\n| Lookup without caching of entities |      0.5-2 |  0.683 us/op |  1.439 ops/us |\n| Lookup without caching of entities |     0.05-2 |  1.114 us/op |  0.928 ops/us |\n| Lookup without caching of entities |    0.005-2 |  1.423 us/op |  0.668 ops/us |\n| Lookup without caching of entities |   0.0005-2 |  2.057 us/op |  0.473 ops/us |\n| Open, query, close cycles          |      0.5-2 | 13.542 us/op |  0.078 ops/us |\n| Open, query, close cycles          |     0.05-2 | 21.221 us/op |  0.048 ops/us |\n| Open, query, close cycles          |    0.005-2 | 36.250 us/op |  0.027 ops/us |\n| Open, query, close cycles          |   0.0005-2 | 84.306 us/op |  0.013 ops/us |\n| Random lon+lat pair generation     |        n/a |  0.047 us/op | 21.297 ops/us |\n\nAs you can see, there is a benefit from using the caching included, and the setup cost of\nthe memory map is not negligible - it takes 50 times as long to open the file than to query.\n\nThe benchmark queries *random* coordinates. Some of these will be at the poles and in oceans.\nThe index is designed to have very stable lookup times, but lookups in asia tend to be slightly\nslower than lookups in America due to the run-length encoding. For most applications, the\ndifferences should remain neglible. In particular, the runtime is expected to increase\nsublinear with the data resolution, not quadratic as with a naive map.\n\nThe last line in above table benchmarks the random number generation. This runtime can\ntherefore be subtracted from the other values.\nOn the 0.005-2 resolution data set, a cached lookup therefore is about 924 ns.\nIn one second, we can perform over 1 million lookups. (1 ops/us = 1 million ops/s)\n\nCaching is not applied to coordinates, but to the decoding of UTF-8,\n`\'\\0\'`-delimited data into\nan array of Java strings. As you can see, the string operations take about 400-500 us/op.\nIf your code is natively operating on UTF-8 encoded,\n0-terminated strings (e.g. C code) then this is not needed.\n\nLookup times increase sub-linear with the data set size:\n\n|Method                           | Resolution | Table size | Runtime     |\n|:--------------------------------|-----------:|-----------:|------------:|\n| Random lon+lat pair generation  |            |            | 0.047 us/op |\n| Lookup with caching of entities |      0.5-2 |   1.602 MB | 0.232 us/op |\n| Lookup with caching of entities |     0.05-2 |   9.775 MB | 0.575 us/op |\n| Lookup with caching of entities |    0.005-2 |  49.098 MB | 0.971 us/op |\n| Lookup with caching of entities |   0.0005-2 | 389.979 MB | 1.562 us/op |\n\n\n## OSM Data Extraction\n\nWe do a multi-pass process to build the index. **Unless you want to rebuild the index,\nyou can stop here. Everything you need to query is documented above.**\n\n1. In the first pass, we ignore all nodes (the majority of the data).\nWe remember all ways and all relations we are interested in,\nbut no additional metadata to conserve memory.\n2. We then build an index of the nodes we will need, and forget ways that we did not use.\n3. In the second pass, we look at the nodes, but only keep those that we are\ninterested in. Since we only need a subset, this should fit into memory now\n(at least if you have a machine with a lot of memory, like I do.)\n4. In the third pass (since ways or nodes might be out of sequence), we\nthen can output the polygons for each relation, along with some metadata.\n\nOn my system, the each pass takes about 2 minutes (reading from a network share;\nlikely a lot faster if I had stored the source file on my SSD).\n\n### Data Structures\n\nWe use Fastutil collections to conserve memory. These classes are excellent\nhashmaps for *primitive* data types. For nodes, we also use a two level hashmap\nwith prefix compression, since node ids were given in sequence not randomly (and thus\nhave a lot of common prefixes - in particular, the first 20+ bits of each id are usually 0).\n\nSince our desired output resolution is much less than 0.01 degree, we also encode\neach coordinate approximately using a single integer.\n\n### Implementation Notes\n\nWhile `osmosis --used-way --used-node` did not work for me with tag filters, it\napparently worked just fine without. Using these filters can reduce the\nplanet file substantially, from 30 GB to 5.1 GB. This is worth\ndoing as a preprocessing step.\n\nAs of now, you *will* need to use a Debian Linux system.\nSome of the libraries are not available on Maven central, so I had to put\nsystem paths (have a look at the pom.xml, for what you need).\n\n## Index construction\n\nThe index essentially is a large pixmap referencing metadata from OSM, accompanied\nwith a table containing the metadata from the index.\n\n### Rendering\n\nRendering is currently done via JavaFX, so you will also need to have a UI for\nbuilding an index. Unfortunately, this is also rather slow (10-30 minutes,\ndepending on the desired resolution and number of polygons to render). However,\nwe needed an API that can render polygons with the even-odd rule and antialiasing,\nand the java-gnome Cairo API wouldn\'t allow us access the resulting bitmaps without\nwriting them to disk as PNG.\n\nSince the JavaFX renderer has a texture limit of 8192x8192, we need to render smaller patches\nand combine them to get a high-resolution map.\n\n### File Format\n\nThe file format is designed to be low-level, compact and efficient. It is meant to\nbe used via read-only shared memory mapping, to make best use of operating\nsystem caching capabilities. The compression is less than what you could obtain with\nPNG encoding or GZIP, but it allows skipping over data without decoding it into\napplication memory.\n\n1. 4 bytes: magic header that identifies the file format. Currently,\nthis is the code 0x6e06e001, and I will increment the last byte on format changes.\n2. 4 bytes: width of the map in pixel\n3. 4 bytes: height of the map in pixel\n4. 4 bytes: width of the map in degree\n5. 4 bytes: height of the map in degree\n6. 4 bytes: longitude offset of the map in degree (usually +180\xc2\xb0)\n7. 4 bytes: latitude offset of the map in degree\n8. 4 bytes: number of entities (max 0x8000)\n9. height * 4 bytes: file offset of each row (monotone increasing)\n10. nument * 4 bytes: file offset of metadata (monotone)\n11. file size (= end of last entity)\n12. x bytes for each row, as listed before (row encoding: see below)\n13. x bytes for each metadata\n\nEach map row is encoded using a run-length encoding consisting of two varints.\nThe first varint is the entity number, the second is the run length - 1.\nThe varint encoding uses a variable number of bytes, where the first bit is 1\nexcept for the last byte.\n\nTo compute the length of the metadata, read two consecutive index offsets and\ncompute the difference. Entries are UTF-8 encoded, and are both separated and\nterminated with `\\0` to make it easy to use them across different programming\nlanguages.  Metadata usually includes tab characters to separate columns. The\nexact column layout is not specified in this file format, but there is\n[documentation in the data folder](data/) for the example data files provided.\n\nThis format is designed to provide reasonable compression, while still allowing\nfast random access without having to decompress the full data. The UTF-8\nencoded entities will often require decoding, and thus the sample implementation\nincludes a cache for the decoded strings.\n\n## Visualization\n\nThe index construction will also produce a .png visualizing the map, as shown above.\n\n## Improving the Data\n\nI am aware there are areas where the data is not yet very good. For example in Portugal,\nthere is little detailed information, same for Western Australia. You are welcome to\ncontribute data: just contribute administrative boundaries\nto [OpenStreetMap](http://www.openstreetmap.org/)!\nFor example there is a project to\n[add administrative boundaries for Portugal](http://wiki.openstreetmap.org/wiki/WikiProject_Portugal/Divis%C3%B5es_Administrativas/Lista_de_Divis%C3%B5es_Administrativas),\nthat already improved the data quality of this index for Portugal substantially.\nIsn\'t that great?\n\n## TODO\n\n1. Include ocean information\n2. Fallback to other data for e.g. Western Australia?\n3. Further reduce file size / coding (but not at the cost of speed)\n4. Use a custom renderer (line-based with RLE to reduce memory?) instead of JavaFX\n\n## Licensing\n\nThe index \'\'query\'\' code is using the liberal BSD 2-clause license.\n\nThe index \'\'construction\'\' code is AGPL-3 licensed (see [LICENSE](LICENSE)).\nI am aware this is a rather restrictive license, but I believe in Copyleft and the GPL:\nbecause I shared my code with you, you should also re-share your improvements, please.\n\nThe \'\'data\'\' is derived from OpenStreetmap, and thus under the\n[Open Data Commons Open Database License](http://www.openstreetmap.org/copyright)\nand you are required to give credit as "\xc2\xa9 OpenStreetMap contributors".\n\n'